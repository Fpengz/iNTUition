"""LLM Provider Abstraction Layer for Aura."""

import asyncio
import logging
import time
from abc import ABC, abstractmethod
from collections.abc import AsyncGenerator
from dataclasses import dataclass
from typing import Any

from ollama import Client
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

logger = logging.getLogger(__name__)


@dataclass
class AuraResponse:
    """Standardized response format for all LLM providers.

    Attributes:
        content: The text generated by the LLM.
        raw_response: The original response object from the provider's SDK.
        usage: A dictionary containing token usage information.
    """

    content: str
    raw_response: Any
    usage: dict[str, int]


class BaseProvider(ABC):
    """Abstract base class for all LLM providers."""

    @abstractmethod
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content based on a prompt."""
        pass

    @abstractmethod
    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Generates content in a streaming fashion."""
        pass


class OllamaProvider(BaseProvider):
    """Local inference using Ollama."""

    def __init__(self, host: str, model: str):
        """Initializes the Ollama provider."""
        self.client = Client(host=host)
        self.model = model

    @retry(
        wait=wait_exponential(multiplier=1, min=2, max=10),
        stop=stop_after_attempt(3),
        reraise=True
    )
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content using Ollama's local API."""
        logger.info(f"Ollama: Generating content with model={self.model} | Temp={temperature}")
        start_time = time.perf_counter()
        options = {
            "temperature": temperature,
            "num_predict": max_tokens,
        }
        try:
            response = self.client.generate(model=self.model, prompt=prompt, options=options)
            duration = time.perf_counter() - start_time
            logger.info(f"Ollama: Generation complete | Duration={duration:.4f}s")
            return AuraResponse(
                content=str(response.get("response", "")),
                raw_response=response,
                usage={"total_tokens": response.get("eval_count", 0)},
            )
        except Exception as e:
            logger.error(f"Ollama error: {e}", exc_info=True)
            raise

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Streams content using Ollama's local API."""
        logger.info(f"Ollama: Starting stream with model={self.model}")
        options = {
            "temperature": temperature,
            "num_predict": max_tokens,
        }
        try:
            for chunk in self.client.generate(model=self.model, prompt=prompt, options=options, stream=True):
                yield chunk.get("response", "")
                await asyncio.sleep(0)
        except Exception as e:
            logger.error(f"Ollama stream error: {e}", exc_info=True)
            raise


class GeminiProvider(BaseProvider):
    """Google Gemini API."""

    def __init__(self, api_key: str, model: str = "gemini-2.0-flash"):
        """Initializes the Gemini provider."""
        from google import genai

        self.client = genai.Client(api_key=api_key)
        self.model = model

    @retry(
        wait=wait_exponential(multiplier=1, min=2, max=10),
        stop=stop_after_attempt(3),
        retry=retry_if_exception_type(Exception),
        reraise=True
    )
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content using Google's GenAI SDK."""
        logger.info(f"Gemini: Generating content with model={self.model}")
        start_time = time.perf_counter()
        try:
            response = self.client.models.generate_content(
                model=self.model, contents=prompt
            )
            duration = time.perf_counter() - start_time
            logger.info(f"Gemini: Generation complete | Duration={duration:.4f}s")
            return AuraResponse(
                content=str(response.text) if response.text else "",
                raw_response=response,
                usage={},
            )
        except Exception as e:
            logger.error(f"Gemini error: {e}", exc_info=True)
            raise

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Streams content using Google's GenAI SDK."""
        logger.info(f"Gemini: Starting stream with model={self.model}")
        try:
            response = self.client.models.generate_content_stream(
                model=self.model, contents=prompt
            )
            for chunk in response:
                if chunk.text:
                    yield chunk.text
                await asyncio.sleep(0)
        except Exception as e:
            logger.error(f"Gemini stream error: {e}", exc_info=True)
            raise


class OpenAIProvider(BaseProvider):
    """OpenAI API Provider."""

    def __init__(self, api_key: str, model: str = "gpt-4o"):
        """Initializes the OpenAI provider.

        Args:
            api_key: Your OpenAI API key.
            model: The OpenAI model identifier.
        """
        self.api_key = api_key
        self.model = model

    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Placeholder for OpenAI generation."""
        return AuraResponse(
            content=f"OpenAI ({self.model}) not fully implemented yet.",
            raw_response=None,
            usage={},
        )

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Placeholder for OpenAI streaming."""
        yield f"OpenAI ({self.model}) streaming not fully implemented yet."


class AnthropicProvider(BaseProvider):
    """Anthropic (Claude) API Provider."""

    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-latest"):
        """Initializes the Anthropic provider.

        Args:
            api_key: Your Anthropic API key.
            model: The Claude model identifier.
        """
        self.api_key = api_key
        self.model = model

    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Placeholder for Anthropic generation."""
        return AuraResponse(
            content=f"Anthropic ({self.model}) not fully implemented yet.",
            raw_response=None,
            usage={},
        )

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Placeholder for Anthropic streaming."""
        yield f"Anthropic ({self.model}) streaming not fully implemented yet."
