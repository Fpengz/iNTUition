"""LLM Provider Abstraction Layer for Aura."""

import asyncio
import logging
from abc import ABC, abstractmethod
from collections.abc import AsyncGenerator
from dataclasses import dataclass
from typing import Any

from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

logger = logging.getLogger(__name__)


@dataclass
class AuraResponse:
    """Standardized response format for all LLM providers.

    Attributes:
        content: The text generated by the LLM.
        raw_response: The original response object from the provider's SDK.
        usage: A dictionary containing token usage information.
    """

    content: str
    raw_response: Any
    usage: dict[str, int]


class BaseProvider(ABC):
    """Abstract base class for all LLM providers."""

    @abstractmethod
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content based on a prompt."""
        pass

    @abstractmethod
    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Generates content in a streaming fashion."""
        pass


class OllamaProvider(BaseProvider):
    """Local inference using Ollama."""

    def __init__(self, host: str, model: str):
        """Initializes the Ollama provider."""
        from ollama import Client

        self.client = Client(host=host)
        self.model = model

    @retry(
        wait=wait_exponential(multiplier=1, min=2, max=10),
        stop=stop_after_attempt(3),
        reraise=True
    )
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content using Ollama's local API."""
        logger.debug(f"Calling Ollama generate (model: {self.model})")
        options = {
            "temperature": temperature,
            "num_predict": max_tokens,
        }
        response = self.client.generate(model=self.model, prompt=prompt, options=options)
        return AuraResponse(
            content=str(response.get("response", "")),
            raw_response=response,
            usage={"total_tokens": response.get("eval_count", 0)},
        )

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Streams content using Ollama's local API."""
        options = {
            "temperature": temperature,
            "num_predict": max_tokens,
        }
        for chunk in self.client.generate(model=self.model, prompt=prompt, options=options, stream=True):
            yield chunk.get("response", "")
            await asyncio.sleep(0)


class GeminiProvider(BaseProvider):
    """Google Gemini API."""

    def __init__(self, api_key: str, model: str = "gemini-2.0-flash"):
        """Initializes the Gemini provider."""
        from google import genai

        self.client = genai.Client(api_key=api_key)
        self.model = model

    @retry(
        wait=wait_exponential(multiplier=1, min=2, max=10),
        stop=stop_after_attempt(3),
        retry=retry_if_exception_type(Exception),
        reraise=True
    )
    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Generates content using Google's GenAI SDK."""
        logger.debug(f"Calling Gemini generate (model: {self.model})")
        response = self.client.models.generate_content(
            model=self.model, contents=prompt
        )
        return AuraResponse(
            content=str(response.text) if response.text else "",
            raw_response=response,
            usage={},
        )

    async def generate_stream(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """Streams content using Google's GenAI SDK."""
        response = self.client.models.generate_content_stream(
            model=self.model, contents=prompt
        )
        for chunk in response:
            if chunk.text:
                yield chunk.text
            await asyncio.sleep(0)


class OpenAIProvider(BaseProvider):
    """OpenAI API Provider."""

    def __init__(self, api_key: str, model: str = "gpt-4o"):
        """Initializes the OpenAI provider.

        Args:
            api_key: Your OpenAI API key.
            model: The OpenAI model identifier.
        """
        self.api_key = api_key
        self.model = model

    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Placeholder for OpenAI generation."""
        return AuraResponse(
            content=f"OpenAI ({self.model}) not fully implemented yet.",
            raw_response=None,
            usage={},
        )


class AnthropicProvider(BaseProvider):
    """Anthropic (Claude) API Provider."""

    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-latest"):
        """Initializes the Anthropic provider.

        Args:
            api_key: Your Anthropic API key.
            model: The Claude model identifier.
        """
        self.api_key = api_key
        self.model = model

    async def generate(
            self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> AuraResponse:
        """Placeholder for Anthropic generation."""
        return AuraResponse(
            content=f"Anthropic ({self.model}) not fully implemented yet.",
            raw_response=None,
            usage={},
        )