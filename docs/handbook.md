26iNTUtionHandbook 5th - 8th FEBRUARY 2026Sponsored by:contentsTable ofTimelineINTUtion0201Problem StatementJudging CriteriaSubmissionPrizesFAQUseful ResourcesContact0304060811121318Presentation1002TIMELINE5TH FEBRUARY, 6PMOPENING CEREMONY5TH FEBRUARY, 6.30PMPROBLEM STATEMENTREVEAL AND HACKINGBEGINS!DINNERHACKINGENDS6TH FEBRUARY, 6.30 PM5TH FEBRUARY, 7.30PMROUND 1JUDGING BEGINS6TH FEBRUARY, 7PM03TIMELINEFINALE OPENINGCEREMONY AND LUNCH!8TH FEBRUARY, 12.30PM7TH FEBRUARY, 11AMRELEASE OF FINALISTSPRESENTATION FORFINAL ROUND BEGINS..8TH FEBRUARY, 1.30PM-4.30PMANNOUNCEMENT OFWINNERS AND CLOSINGCEREMONY!8TH FEBRUARY, 5.30PMWHAT ISiNTUtion?INTUtion 2026 is an in-person hackathon focused onbuilding innovative AI-driven solutions to real-worldproblem statements. Organized by the IEEE NTUStudent Chapter, the event brings together passionatestudent teams to design, prototype, and presentimpactful ideas. INTUtion aims to help participantssharpen their technical, problem-solving, and pitchingskills while gaining exposure to industry-relevantchallenges and judging standards.04PROBLEMSTATEMENT Despite rapid digital innovation, many digital interfaces remaininaccessible due to rigid, one-size-fits-all designs that fail toadapt to diverse user abilities. Poor compatibility with assistivetechnologies, high cognitive load from complex interfaces, anda lack of multimodal alternatives prevent users with visual,auditory, motor, or cognitive impairments from using digital toolsindependently and effectively. These issues are compounded bylimited personalization options and inconsistent accessibilitysupport across platforms and updates, turning everyday digitalinteractions into significant barriers rather than enablers.Submissions are expected to go beyond concepts or UImockups and instead provide a functional, end-to-end system,  involving multimodal interaction (e.g. text, speech, vision, or UIadaptation), that can be executed, evaluated, anddemonstrated live. 05PROBLEMSTATEMENTThe goal is to let teams clearly define the interface accessibilityproblem they are addressing, justify the use of AI, and deliver asolution that is usable, testable, and presentable.06Examples:1.Adaptive UI mode: Automatically adjusts layout, contrast, textsize, and touch targets to fit different user needs, addressingrigid one-size-fits-all interface designs. (Example Tech stack:React / Next.js, Tailwind CSS, CSS media queries, JavaScriptaccessibility APIs, WCAG-compliant ARIA attributes, devicesensor APIs (where applicable))2.Multimodal content converter: Presents the same informationin text, audio, and visual formats, reducing exclusion caused bysingle-modality content.3.Accessibility health checker: Detects common accessibilityissues in real time and suggests fixes, improving compatibilitywith assistive technologies*These examples are to support brainstorming and to provide abetter overview of the problem statement. They are not concreteideas that you must choose from.CriterionDescriptionImpact (25%)Is the specific accessibility barrier (visual, auditory, motor, cognitive) clearlyidentified and grounded in a real user scenario?Does the solution directly address a failure of rigid, one-size-fits-allinterfaces described in the problem statement?Does the system measurably reduce user burden (e.g. cognitive load,interaction steps, error rate, time-to-completion)?Are improvements demonstrated relative to a baseline interface (before vsafter)?Does the solution enable users to complete tasks independently withoutexternal assistance?Does it reduce reliance on manual accessibility settings or staticconfigurations?Real-timeperformance &latency  (25%)How quickly does the system process and respond to multimodal inputs(speech, text, vision, interaction signals)?Are interactions fast enough to avoid breaking usability for users withimpairments?Are latency–accuracy trade-offs explicitly justified for accessibility (e.g.faster feedback over perfect transcription)?Does the system degrade gracefully under slower networks or weakerdevices?Does the system remain stable during continuous interaction, not just singlerequests?Can it handle concurrent users or repeated interactions without failure?Design (25%)Is this a fully functional pipeline, not disconnected components?Are AI models, backend services, and UI layers cleanly integrated?Does the interface adapt dynamically to user needs rather than relying onfixed accessibility settings?Are adaptations understandable, predictable, and reversible for the user?Is the UI intuitive and usable for the target impairment group?Are design choices (contrast, layout, interaction mode) justified based onaccessibility needs?Does the system work reliably during a live demo without manualintervention?Is the solution clearly presented, reproducible, and technically sound?Innovation &Creativity (25%)Does the solution move beyond standard accessibility features (screenreaders, captions, dark mode)?Is AI used in a creative or unexpected way to solve the accessibility problem?Is there a clear “wow factor” rooted in accessibility impact, not just technicalcomplexity?Does the solution meaningfully stand out from common accessibility tooling?Does the system show clear paths for extension to other impairments,interfaces, or platforms?Can the approach evolve with more data, users, or modalities?JudgingCriteria07Criterion9–10(Excellent)6–8 (Good)3–5 (Fair)0–2 (Poor)Impact(25%)Clearly definedaccessibilitybarrier; strongreduction in usereffort or cognitiveload; enablesindependent use;broadly applicableacross users orplatformsClear problemdefinition; visibleaccessibilityimprovement;supports partialindependence;limitedgeneralizationProblem looselydefined; minoror indirectaccessibilitybenefit; narrowscopeNo clearaccessibilityproblem; purelycosmeticchanges; noreal benefitReal-timePerformance& Latency(25%)Responsivemultimodalinteraction; low orwell-justifiedlatency; stableunder continuoususe; suitable forlive deploymentGenerallyresponsive;occasionaldelays; usablein mostscenariosNoticeable lag;inconsistentresponsiveness;degradesusabilityNon-interactiveor batch-based; latencybreaks usability;unstableDesign (25%)Fully functionalend-to-endsystem; well-integratedcomponents;adaptive behavioraligned to userneeds; reliable livedemoFunctionalsystem;reasonableintegration;some adaptiveelements; demomostly worksPartialimplementation; weakintegration;limitedadaptivity;fragile demoUI mockup orstaticprototype;disconnectedparts; demofailsInnovation &Creativity(25%)AI meaningfullyenablespersonalization,adaptation, ormultimodality;non-obviousapproach; strongdifferentiation;clear extensionpotentAI adds value;moderatenovelty; somedifferentiationConventionalapproach; AIlargelyreplaceable byrules orheuristicsNo clear AI role;gimmicky orunnecessaryuse of AIJudgingCriteria08SUBMISSIONFORMAT09Teams must submit a valid github link that mustinclude clear execution instructions andrepository structure in a readme.md file. If theproject is not executable by judges due to anyreason like api keys, private tokens, etc.. teamsmust provide a valid working deployed web-app link.Teams must also submit a supporting pitchdocument. This can either be a videopresentation (2 minutes maximum) or a slidedeck PDF (12 slides maximum). Highlight theaccessibility problem, describe the solution andits features based on the judging criteria.SUBMISSION PORTALSubmit the above on the google forms linkprovided below. Submit the video as a sharedgoogle drive link or an unlisted youtube videolink. For text documents, submit a pdf.https://shorturl.at/n0mCBSUBMISSIONROUND 2 10Top 10 teams selected for Round 2 can refineand polish their end products for the finalpresentation.There is no submission for the final round, onlya presentation.PRESENTATION GUIDELINESThe presentation must be 8-10 minutes long,including a live demo showcasing yourdeveloped idea. No restrictions on length ofslide deck.After the presentation, there will be 5 minutesfor a Q&A session where judges will asktechnical and non-technical questionsregarding your idea.PRIZES11$2000$1000$500$500 for best freshmanTrustGood Luck!12USEFUL RESOURCES13MultiModal14Multimodal learning integrates data from differentmodalities, such as text, images, audio, and video,enabling advanced AI models to understand andprocess information across various contexts. Belowis an overview of commonly used datasets andmodels in multimodal AI.Note that use of MultiModal is neither mandatorynor will make a difference in judging criterion aslong as the solution is creatively and innovativelysolving the problem statement.DATASETS15LAION-5BA large-scale dataset containing 5 billion image-textpairs sourced from the web, used for training models likeStable Diffusion for tasks such as image generation andcross-modal retrieval.COCO (Common Objects in Context)A dataset of 300,000 labeled images with captions,commonly used for image captioning, object detection,and scene understanding.Visual GenomeProvides 108,000 images annotated with objects,attributes, relationships, and region-specific captions,focusing on scene graph generation and semanticunderstanding.VQA (Visual Question Answering)A dataset of images paired with natural languagequestions and answers, aimed at evaluating models onvisual-text reasoning tasks.AudioSetContains over 2 million 10-second audio clips labeledwith sound events, supporting applications like audio-visual synchronization and sound recognition.MultiModal16CLIP (Contrastive Language-Image Pretraining)Associates images and text by learning sharedembeddings, enabling zero-shot classification,content-based retrieval, and captioning.DALL·EGenerates high-quality, diverse images fromtextual descriptions using multimodalembeddings, ideal for creative contentgeneration.BLIP (Bootstrapped Language-Image Pretraining)Combines image and language embeddings fortasks like captioning and retrieval, offeringefficient multimodal solutions.VisualBERTIntegrates image region embeddings with textusing a transformer architecture for tasks likequestion answering and reasoning.VideoCLIPExtends CLIP to video by adding temporalinformation, enabling video understanding, actionrecognition, and text-to-video retrieval.UsefulResources17OpenCLIPAn open-source implementation of OpenAI's CLIP,this project has trained several models onvarious data sources and compute budgets,ranging from small-scale experiments to largerruns, including models trained on datasets suchas LAION-400M, LAION-2B, and DataComp-1B.Multimodal AI02An article that explores thefundamentals of multimodal AI, itsapplications, and the integration ofdifferent data types for comprehensiveunderstanding:https://www.datastax.com/guides/multimodal-ai?utm_source=chatgpt.com01Contact Us 18All communication will be via Email or Telegram.  Make sure that you have joined the telegram chat below: Email: IEEENTU-Branch@e.ntu.edu.sgTele: https://t.me/+NIkz-sRyntE1MzM1 or QR below:
